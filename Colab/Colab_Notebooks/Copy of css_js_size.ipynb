{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMUoi7ZwwBA+FS2px7iUXdC"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"B-mCU_urP9R7","executionInfo":{"status":"ok","timestamp":1727840713346,"user_tz":-330,"elapsed":19521,"user":{"displayName":"Animesh Awasthi","userId":"06245988644800937794"}},"outputId":"f59cbc19-22d0-4dc9-d397-dda3ae0614af"},"outputs":[{"output_type":"stream","name":"stdout","text":["Enter the URL: https://bekushal.com\n","<Response [200]>\n","200\n","Total CSS Size: 6.73 KB\n","Total JavaScript Size: 0.00 KB\n"]}],"source":["import requests\n","from bs4 import BeautifulSoup\n","from urllib.parse import urljoin\n","\n","def fetch_webpage(url):\n","    if not url.startswith(('http://', 'https://')):\n","        url = 'https://' + url\n","    response = requests.get(url)\n","    print(response)\n","    print(response.status_code)\n","    if response.status_code == 200:\n","        return response.text\n","    else:\n","        return None\n","\n","def parse_html(html_content):\n","    soup = BeautifulSoup(html_content, 'html.parser')\n","    return soup\n","\n","def extract_css_js_files(soup, base_url):\n","    css_files = [urljoin(base_url, link.get('href')) for link in soup.find_all('link', rel='stylesheet')]\n","    js_files = [urljoin(base_url, script.get('src')) for script in soup.find_all('script') if script.get('src')]\n","\n","    # Extract internal CSS and JS\n","    internal_css = [style.string for style in soup.find_all('style') if style.string]\n","    internal_js = [script.string for script in soup.find_all('script') if script.string]\n","\n","    return css_files, js_files, internal_css, internal_js\n","\n","def get_file_size(url):\n","    response = requests.head(url)\n","    if response.status_code == 200:\n","        content_length = response.headers.get('Content-Length')\n","        if content_length:\n","            return int(content_length)\n","        else:\n","            # Fallback to GET request if Content-Length is not provided\n","            response = requests.get(url)\n","            return len(response.content)\n","    return 0\n","\n","def evaluate_file_sizes(css_files, js_files, internal_css, internal_js):\n","    total_css_size = sum(get_file_size(url) for url in css_files) + sum(len(css) for css in internal_css)\n","    total_js_size = sum(get_file_size(url) for url in js_files) + sum(len(js) for js in internal_js)\n","    return total_css_size, total_js_size\n","\n","def main(url):\n","    html_content = fetch_webpage(url)\n","    if html_content:\n","        soup = parse_html(html_content)\n","        css_files, js_files, internal_css, internal_js = extract_css_js_files(soup, url)\n","        total_css_size, total_js_size = evaluate_file_sizes(css_files, js_files, internal_css, internal_js)\n","        print(f\"Total CSS Size: {total_css_size / 1024:.2f} KB\")\n","        print(f\"Total JavaScript Size: {total_js_size / 1024:.2f} KB\")\n","    else:\n","        print(\"Failed to fetch the webpage.\")\n","\n","if __name__ == \"__main__\":\n","    url = input(\"Enter the URL: \")\n","    main(url)\n"]},{"cell_type":"code","source":["import requests\n","from bs4 import BeautifulSoup\n","from urllib.parse import urljoin\n","import time\n","\n","def fetch_webpage(url, retries=3, delay=5):\n","    headers = {\n","        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n","    }\n","    for attempt in range(retries):\n","        response = requests.get(url, headers=headers)\n","        if response.status_code == 200:\n","            return response.text\n","        elif response.status_code == 503:\n","            print(f\"503 Service Unavailable. Retrying in {delay} seconds...\")\n","            time.sleep(delay)\n","        else:\n","            return None\n","    return None\n","\n","def parse_html(html_content):\n","    soup = BeautifulSoup(html_content, 'html.parser')\n","    return soup\n","\n","def extract_css_js_files(soup, base_url):\n","    css_files = [urljoin(base_url, link.get('href')) for link in soup.find_all('link', rel='stylesheet')]\n","    js_files = [urljoin(base_url, script.get('src')) for script in soup.find_all('script') if script.get('src')]\n","\n","    # Extract internal CSS and JS\n","    internal_css = [style.string for style in soup.find_all('style') if style.string]\n","    internal_js = [script.string for script in soup.find_all('script') if script.string]\n","\n","    return css_files, js_files, internal_css, internal_js\n","\n","def get_file_size(url):\n","    headers = {\n","        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n","    }\n","    response = requests.head(url, headers=headers)\n","    if response.status_code == 200:\n","        content_length = response.headers.get('Content-Length')\n","        if content_length:\n","            return int(content_length)\n","        else:\n","            # Fallback to GET request if Content-Length is not provided\n","            response = requests.get(url, headers=headers)\n","            return len(response.content)\n","    return 0\n","\n","def evaluate_file_sizes(css_files, js_files, internal_css, internal_js):\n","    total_css_size = sum(get_file_size(url) for url in css_files) + sum(len(css) for css in internal_css)\n","    total_js_size = sum(get_file_size(url) for url in js_files) + sum(len(js) for js in internal_js)\n","    return total_css_size, total_js_size\n","\n","def main(url):\n","    html_content = fetch_webpage(url)\n","    if html_content:\n","        soup = parse_html(html_content)\n","        css_files, js_files, internal_css, internal_js = extract_css_js_files(soup, url)\n","        total_css_size, total_js_size = evaluate_file_sizes(css_files, js_files, internal_css, internal_js)\n","        print(f\"Total CSS Size: {total_css_size / 1024:.2f} KB\")\n","        print(f\"Total JavaScript Size: {total_js_size / 1024:.2f} KB\")\n","    else:\n","        print(\"Failed to fetch the webpage.\")\n","\n","if __name__ == \"__main__\":\n","    url = input(\"Enter the URL: \")\n","    main(url)\n"],"metadata":{"id":"YT987ldSmFqf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1727840757376,"user_tz":-330,"elapsed":24513,"user":{"displayName":"Animesh Awasthi","userId":"06245988644800937794"}},"outputId":"06dbdc1a-3d08-463d-a018-d28435ca7939"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Enter the URL: https://www.bekushal.com/\n","Total CSS Size: 518.65 KB\n","Total JavaScript Size: 343.45 KB\n"]}]}]}